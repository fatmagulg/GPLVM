# GPLVM
All my work on dimension reduction project including PCA and GPLVM

The aim of this project was to explore how different dimension reduction methods perform on different datasets. In particular,
I chose to compare PCA with GPLVM (Gaussian Process Latent Variable Model) as two DR methods for a number of datasets. 
The Python modules I used were PCA from sklearn.decomposition and the GPy library made by the SheffieldML group.

The first dataset was toy data I generated by writing a function that generates (evenly distributed) points on the surface of a 
sphere. i.e. the points are not random. This is just to see how PCA and GPLVM might map points onto latent space differently 
and to get familiar with the different plotting aesthetics of the libraries. I then wrote a modified function for generating the 
sphere points in which the number of points in the upper and lower hemispheres is different. In the particular file I have uploaded
the ratio between lower:upper is 0.25:0.75 but this can easily be modified as desired. Again this is to explore a little more
how PCA and GPLVM behave differently, this time with different levels of point density in the classes. 

The next dataset used is a dataset relating to breast cancer tumors, which consists of 569 observations of breast tumours with
30 covariates corresponding to different features of the tumour, such as radius and smoothness. The response is a binary variable
corresponding to whether the tumour was malignant or benign. PCA and GPLVM have been applied to the data to reduce dimensionality 
from 30 to 2 or 3. 

Another dataset that I examined is a dataset of handwritten digits. Handwritten digits are a common type of high-dimensional 
data that are useful for demonstrating dimension reduction methods. The dataset used for this report is a subset of the 
MNIST (Mixed National Institute of Standards and Technology) database. Originally consisting of 70,000 instances, the data has
been subsetted to 100 instances for ease of computation and visualisation. The data consists of 784-dimensions, where each 
dimension is one covariate and corresponds to a single pixel in a 28x28 grid on which a digit from 0-9 has been hand-written. 
The value of each covariate is either 0 (the pixel is white) or 1 (the pixel is shaded).

Conclusions of my work are written in my project report which is not included here at the moment. I will think about uploading 
it or some similar written conclusions later. For now the notebooks include nearest neighbour correct classification rate which 
was one of the criterion I used to decide which DR method is more appropriate for each dataset. By looking at the plots one can 
also decide which method performs better in terms of producing a clearer latent visualisation. 

I admit that this is not amazing work by any means. I had limited time to learn about GP's and GPLVM from scratch and to be honest
the learning resources that are out there on GPLVM and GPy are not very good to put it nicely. I just want to document it anyway
for myself.
